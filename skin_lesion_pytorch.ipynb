{"cells":[{"cell_type":"markdown","metadata":{},"source":["This is a very interesting task and it's very suitable for beginners, through this task you can learn about data analysis, data processing, model building, model training, parameter optimization and so on. When only use the images, with a common network, such as Resnet, Densenet, you can achieve a relatively good accuracy very easily. \n","\n","By analyzing the data, the basic information of the patient is also related to the classification of the diseased tissue. Therefore, if we can combine the case information to carry out the classification task, it will be a very meaningful work. Actually during clinical diagnosis, doctors will also combine different modal data to make comprehensive judgments.\n","\n","Due to the urgency of time, my current method only uses image data, and then I will consider adding the patient's personal information to the classification task to train a more complete model. I will update my kernel immediately once I finished.\n","\n","Before you really start, I strongly recommend you to read the material of pigmented lesions and dermatoscopic images[https://arxiv.org/abs/1803.10417]. After that, you can learn about the characteristics and distribution of the data from the task description and this kernel[https://www.kaggle.com/kmader/dermatology-mnist-loading-and-processing]\n","\n","In this kernel I have followed following steps for model building and evaluation: \n","\n","> Step 1. Data analysis and preprocessing\n","\n","> Step 2. Model building\n","\n","> Step 3. Model training\n","\n","> Step 4. Model evaluation\n","\n","I used the pytorch framework to complete the entire task. The code contains several common networks, such as Resnet, VGG, Densenet, and Inception. You only need to make minor changes on the code to complete the network switch. Without the hyperparameter adjustment, I used **Densenet-121 to achieve an accuracy of more than 90% on the validation set in 10 epochs.**\n","\n"]},{"cell_type":"markdown","metadata":{},"source":[" ### First, import all libraries that used in this project"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"outputs":[],"source":["%matplotlib inline\n","# python libraties\n","import os, cv2,itertools\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import pandas as pd\n","from tqdm import tqdm\n","from glob import glob\n","from PIL import Image\n","\n","# pytorch libraries\n","import torch\n","from torch import optim,nn\n","from torch.autograd import Variable\n","from torch.utils.data import DataLoader,Dataset\n","from torchvision import models,transforms\n","\n","# sklearn libraries\n","from sklearn.metrics import confusion_matrix\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import classification_report\n","\n","# to make the results are reproducible\n","np.random.seed(10)\n","torch.manual_seed(10)\n","torch.cuda.manual_seed(10)\n","\n","print(os.listdir(\"../input\"))"]},{"cell_type":"markdown","metadata":{},"source":["## Step 1. Data analysis and preprocessing"]},{"cell_type":"markdown","metadata":{},"source":["Get the all image data pathsï¼Œ match the row information in HAM10000_metadata.csv with its corresponding image"]},{"cell_type":"code","execution_count":null,"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"outputs":[],"source":["data_dir = '../input'\n","all_image_path = glob(os.path.join(data_dir, '*', '*.jpg'))\n","imageid_path_dict = {os.path.splitext(os.path.basename(x))[0]: x for x in all_image_path}\n","lesion_type_dict = {\n","    'nv': 'Melanocytic nevi',\n","    'mel': 'dermatofibroma',\n","    'bkl': 'Benign keratosis-like lesions ',\n","    'bcc': 'Basal cell carcinoma',\n","    'akiec': 'Actinic keratoses',\n","    'vasc': 'Vascular lesions',\n","    'df': 'Dermatofibroma'\n","}"]},{"cell_type":"markdown","metadata":{},"source":["This function is used to compute the mean and standard deviation on the whole dataset, will use for inputs normalization"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def compute_img_mean_std(image_paths):\n","    \"\"\"\n","        computing the mean and std of three channel on the whole dataset,\n","        first we should normalize the image from 0-255 to 0-1\n","    \"\"\"\n","\n","    img_h, img_w = 224, 224\n","    imgs = []\n","    means, stdevs = [], []\n","\n","    for i in tqdm(range(len(image_paths))):\n","        img = cv2.imread(image_paths[i])\n","        img = cv2.resize(img, (img_h, img_w))\n","        imgs.append(img)\n","\n","    imgs = np.stack(imgs, axis=3)\n","    print(imgs.shape)\n","\n","    imgs = imgs.astype(np.float32) / 255.\n","\n","    for i in range(3):\n","        pixels = imgs[:, :, i, :].ravel()  # resize to one row\n","        means.append(np.mean(pixels))\n","        stdevs.append(np.std(pixels))\n","\n","    means.reverse()  # BGR --> RGB\n","    stdevs.reverse()\n","\n","    print(\"normMean = {}\".format(means))\n","    print(\"normStd = {}\".format(stdevs))\n","    return means,stdevs"]},{"cell_type":"markdown","metadata":{},"source":["Return the mean and std of RGB channels"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["norm_mean,norm_std = compute_img_mean_std(all_image_path)"]},{"cell_type":"markdown","metadata":{},"source":["Add three columns to the original DataFrame, path (image path), cell_type (the whole name),cell_type_idx (the corresponding index  of cell type, as the image label )"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_original = pd.read_csv(os.path.join(data_dir, 'HAM10000_metadata.csv'))\n","df_original['path'] = df_original['image_id'].map(imageid_path_dict.get)\n","df_original['cell_type'] = df_original['dx'].map(lesion_type_dict.get)\n","df_original['cell_type_idx'] = pd.Categorical(df_original['cell_type']).codes\n","df_original.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this will tell us how many images are associated with each lesion_id\n","df_undup = df_original.groupby('lesion_id').count()\n","# now we filter out lesion_id's that have only one image associated with it\n","df_undup = df_undup[df_undup['image_id'] == 1]\n","df_undup.reset_index(inplace=True)\n","df_undup.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# here we identify lesion_id's that have duplicate images and those that have only one image.\n","def get_duplicates(x):\n","    unique_list = list(df_undup['lesion_id'])\n","    if x in unique_list:\n","        return 'unduplicated'\n","    else:\n","        return 'duplicated'\n","\n","# create a new colum that is a copy of the lesion_id column\n","df_original['duplicates'] = df_original['lesion_id']\n","# apply the function to this new column\n","df_original['duplicates'] = df_original['duplicates'].apply(get_duplicates)\n","df_original.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_original['duplicates'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# now we filter out images that don't have duplicates\n","df_undup = df_original[df_original['duplicates'] == 'unduplicated']\n","df_undup.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# now we create a val set using df because we are sure that none of these images have augmented duplicates in the train set\n","y = df_undup['cell_type_idx']\n","_, df_val = train_test_split(df_undup, test_size=0.2, random_state=101, stratify=y)\n","df_val.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_val['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# This set will be df_original excluding all rows that are in the val set\n","# This function identifies if an image is part of the train or val set.\n","def get_val_rows(x):\n","    # create a list of all the lesion_id's in the val set\n","    val_list = list(df_val['image_id'])\n","    if str(x) in val_list:\n","        return 'val'\n","    else:\n","        return 'train'\n","\n","# identify train and val rows\n","# create a new colum that is a copy of the image_id column\n","df_original['train_or_val'] = df_original['image_id']\n","# apply the function to this new column\n","df_original['train_or_val'] = df_original['train_or_val'].apply(get_val_rows)\n","# filter out train rows\n","df_train = df_original[df_original['train_or_val'] == 'train']\n","print(len(df_train))\n","print(len(df_val))"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_train['cell_type_idx'].value_counts()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["df_val['cell_type'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["**From From the above statistics of each category, we can see that there is a serious class imbalance in the training data. To solve this problem, I think we can start from two aspects, one is equalization sampling, and the other is a loss function that can be used to mitigate category imbalance during training, such as focal loss.**"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Copy fewer class to balance the number of 7 classes\n","data_aug_rate = [15,10,5,50,0,40,5]\n","for i in range(7):\n","    if data_aug_rate[i]:\n","        df_train=df_train.append([df_train.loc[df_train['cell_type_idx'] == i,:]]*(data_aug_rate[i]-1), ignore_index=True)\n","df_train['cell_type'].value_counts()"]},{"cell_type":"markdown","metadata":{},"source":["At the beginning, I divided the data into three parts, training set, validation set and test set. Considering the small amount of data, I did not further divide the validation set data in practice."]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# # We can split the test set again in a validation set and a true test set:\n","# df_val, df_test = train_test_split(df_val, test_size=0.5)\n","df_train = df_train.reset_index()\n","df_val = df_val.reset_index()\n","# df_test = df_test.reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["## Step 2. Model building"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# feature_extract is a boolean that defines if we are finetuning or feature extracting. \n","# If feature_extract = False, the model is finetuned and all model parameters are updated. \n","# If feature_extract = True, only the last layer parameters are updated, the others remain fixed.\n","def set_parameter_requires_grad(model, feature_extracting):\n","    if feature_extracting:\n","        for param in model.parameters():\n","            param.requires_grad = False"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def initialize_model(model_name, num_classes, feature_extract, use_pretrained=True):\n","    # Initialize these variables which will be set in this if statement. Each of these\n","    #   variables is model specific.\n","    model_ft = None\n","    input_size = 0\n","\n","    if model_name == \"resnet\":\n","        \"\"\" Resnet18, resnet34, resnet50, resnet101\n","        \"\"\"\n","        model_ft = models.resnet50(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","\n","    elif model_name == \"vgg\":\n","        \"\"\" VGG11_bn\n","        \"\"\"\n","        model_ft = models.vgg11_bn(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier[6].in_features\n","        model_ft.classifier[6] = nn.Linear(num_ftrs,num_classes)\n","        input_size = 224\n","\n","\n","    elif model_name == \"densenet\":\n","        \"\"\" Densenet121\n","        \"\"\"\n","        model_ft = models.densenet121(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        num_ftrs = model_ft.classifier.in_features\n","        model_ft.classifier = nn.Linear(num_ftrs, num_classes)\n","        input_size = 224\n","\n","    elif model_name == \"inception\":\n","        \"\"\" Inception v3\n","        Be careful, expects (299,299) sized images and has auxiliary output\n","        \"\"\"\n","        model_ft = models.inception_v3(pretrained=use_pretrained)\n","        set_parameter_requires_grad(model_ft, feature_extract)\n","        # Handle the auxilary net\n","        num_ftrs = model_ft.AuxLogits.fc.in_features\n","        model_ft.AuxLogits.fc = nn.Linear(num_ftrs, num_classes)\n","        # Handle the primary net\n","        num_ftrs = model_ft.fc.in_features\n","        model_ft.fc = nn.Linear(num_ftrs,num_classes)\n","        input_size = 299\n","\n","    else:\n","        print(\"Invalid model name, exiting...\")\n","        exit()\n","    return model_ft, input_size"]},{"cell_type":"markdown","metadata":{},"source":["You can change your backbone network, here are 4 different networks, each network also has sevaral versions. Considering the limited training data, we used the ImageNet pre-training model for fine-tuning. This can speed up the convergence of the model and improve the accuracy.\n","\n","There is one thing you need to pay attention to, the input size of Inception is different from the others (299x299), you need to change the setting of compute_img_mean_std() function "]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# resnet,vgg,densenet,inception\n","model_name = 'densenet'\n","num_classes = 7\n","feature_extract = False\n","# Initialize the model for this run\n","model_ft, input_size = initialize_model(model_name, num_classes, feature_extract, use_pretrained=True)\n","# Define the device:\n","device = torch.device('cuda:0')\n","# Put the model on the device:\n","model = model_ft.to(device)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# norm_mean = (0.49139968, 0.48215827, 0.44653124)\n","# norm_std = (0.24703233, 0.24348505, 0.26158768)\n","# define the transformation of the train images.\n","train_transform = transforms.Compose([transforms.Resize((input_size,input_size)),transforms.RandomHorizontalFlip(),\n","                                      transforms.RandomVerticalFlip(),transforms.RandomRotation(20),\n","                                      transforms.ColorJitter(brightness=0.1, contrast=0.1, hue=0.1),\n","                                        transforms.ToTensor(), transforms.Normalize(norm_mean, norm_std)])\n","# define the transformation of the val images.\n","val_transform = transforms.Compose([transforms.Resize((input_size,input_size)), transforms.ToTensor(),\n","                                    transforms.Normalize(norm_mean, norm_std)])"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define a pytorch dataloader for this dataset\n","class HAM10000(Dataset):\n","    def __init__(self, df, transform=None):\n","        self.df = df\n","        self.transform = transform\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","    def __getitem__(self, index):\n","        # Load data and get label\n","        X = Image.open(self.df['path'][index])\n","        y = torch.tensor(int(self.df['cell_type_idx'][index]))\n","\n","        if self.transform:\n","            X = self.transform(X)\n","\n","        return X, y"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Define the training set using the table train_df and using our defined transitions (train_transform)\n","training_set = HAM10000(df_train, transform=train_transform)\n","train_loader = DataLoader(training_set, batch_size=32, shuffle=True, num_workers=4)\n","# Same for the validation set:\n","validation_set = HAM10000(df_val, transform=train_transform)\n","val_loader = DataLoader(validation_set, batch_size=32, shuffle=False, num_workers=4)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# we use Adam optimizer, use cross entropy loss as our loss function\n","optimizer = optim.Adam(model.parameters(), lr=1e-3)\n","criterion = nn.CrossEntropyLoss().to(device)"]},{"cell_type":"markdown","metadata":{},"source":["## Step 3. Model training"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# this function is used during training process, to calculation the loss and accuracy\n","class AverageMeter(object):\n","    def __init__(self):\n","        self.reset()\n","\n","    def reset(self):\n","        self.val = 0\n","        self.avg = 0\n","        self.sum = 0\n","        self.count = 0\n","\n","    def update(self, val, n=1):\n","        self.val = val\n","        self.sum += val * n\n","        self.count += n\n","        self.avg = self.sum / self.count"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["total_loss_train, total_acc_train = [],[]\n","def train(train_loader, model, criterion, optimizer, epoch):\n","    model.train()\n","    train_loss = AverageMeter()\n","    train_acc = AverageMeter()\n","    curr_iter = (epoch - 1) * len(train_loader)\n","    for i, data in enumerate(train_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        # print('image shape:',images.size(0), 'label shape',labels.size(0))\n","        images = Variable(images).to(device)\n","        labels = Variable(labels).to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(images)\n","\n","        loss = criterion(outputs, labels)\n","        loss.backward()\n","        optimizer.step()\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        train_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","        train_loss.update(loss.item())\n","        curr_iter += 1\n","        if (i + 1) % 100 == 0:\n","            print('[epoch %d], [iter %d / %d], [train loss %.5f], [train acc %.5f]' % (\n","                epoch, i + 1, len(train_loader), train_loss.avg, train_acc.avg))\n","            total_loss_train.append(train_loss.avg)\n","            total_acc_train.append(train_acc.avg)\n","    return train_loss.avg, train_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def validate(val_loader, model, criterion, optimizer, epoch):\n","    model.eval()\n","    val_loss = AverageMeter()\n","    val_acc = AverageMeter()\n","    with torch.no_grad():\n","        for i, data in enumerate(val_loader):\n","            images, labels = data\n","            N = images.size(0)\n","            images = Variable(images).to(device)\n","            labels = Variable(labels).to(device)\n","\n","            outputs = model(images)\n","            prediction = outputs.max(1, keepdim=True)[1]\n","\n","            val_acc.update(prediction.eq(labels.view_as(prediction)).sum().item()/N)\n","\n","            val_loss.update(criterion(outputs, labels).item())\n","\n","    print('------------------------------------------------------------')\n","    print('[epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, val_loss.avg, val_acc.avg))\n","    print('------------------------------------------------------------')\n","    return val_loss.avg, val_acc.avg"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["epoch_num = 10\n","best_val_acc = 0\n","total_loss_val, total_acc_val = [],[]\n","for epoch in range(1, epoch_num+1):\n","    loss_train, acc_train = train(train_loader, model, criterion, optimizer, epoch)\n","    loss_val, acc_val = validate(val_loader, model, criterion, optimizer, epoch)\n","    total_loss_val.append(loss_val)\n","    total_acc_val.append(acc_val)\n","    if acc_val > best_val_acc:\n","        best_val_acc = acc_val\n","        print('*****************************************************')\n","        print('best record: [epoch %d], [val loss %.5f], [val acc %.5f]' % (epoch, loss_val, acc_val))\n","        print('*****************************************************')"]},{"cell_type":"markdown","metadata":{},"source":["## Step 4. Model evaluation"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["fig = plt.figure(num = 2)\n","fig1 = fig.add_subplot(2,1,1)\n","fig2 = fig.add_subplot(2,1,2)\n","fig1.plot(total_loss_train, label = 'training loss')\n","fig1.plot(total_acc_train, label = 'training accuracy')\n","fig2.plot(total_loss_val, label = 'validation loss')\n","fig2.plot(total_acc_val, label = 'validation accuracy')\n","plt.legend()\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    plt.colorbar()\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, cm[i, j],\n","                 horizontalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["model.eval()\n","y_label = []\n","y_predict = []\n","with torch.no_grad():\n","    for i, data in enumerate(val_loader):\n","        images, labels = data\n","        N = images.size(0)\n","        images = Variable(images).to(device)\n","        outputs = model(images)\n","        prediction = outputs.max(1, keepdim=True)[1]\n","        y_label.extend(labels.cpu().numpy())\n","        y_predict.extend(np.squeeze(prediction.cpu().numpy().T))\n","\n","# compute the confusion matrix\n","confusion_mtx = confusion_matrix(y_label, y_predict)\n","# plot the confusion matrix\n","plot_labels = ['akiec', 'bcc', 'bkl', 'df', 'nv', 'vasc','mel']\n","plot_confusion_matrix(confusion_mtx, plot_labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["# Generate a classification report\n","report = classification_report(y_label, y_predict, target_names=plot_labels)\n","print(report)"]},{"cell_type":"code","execution_count":null,"metadata":{"trusted":true},"outputs":[],"source":["label_frac_error = 1 - np.diag(confusion_mtx) / np.sum(confusion_mtx, axis=1)\n","plt.bar(np.arange(7),label_frac_error)\n","plt.xlabel('True Label')\n","plt.ylabel('Fraction classified incorrectly')"]},{"cell_type":"markdown","metadata":{},"source":["## Conclusion"]},{"cell_type":"markdown","metadata":{},"source":["I tried to train with different network structures. When using Densenet-121, the average accuracy of 7 classes on the validation set can reach 92% in 10 epochs. We also calculated the confusion matrix for all classes and the F1-score for each class, which is a more comprehensive indicator that can take into account both the precision and recall of the classification model.Our model can achieve more than 90% on the F1-score indicator.\n","\n","Due to limited time, we did not spend much time on model training. By increasing in training epochs, adjustmenting of model hyperparameters, and attempting at different networks may further enhance the performance of the model."]},{"cell_type":"markdown","metadata":{},"source":["## Next plan"]},{"cell_type":"markdown","metadata":{},"source":["How to use image data and patient case data at the same time, my plan is to use CNN to extract features from images, use xgboost to convert medical records into vectors and then concat them with CNN network full-layer features. Two branch networks are trained simultaneously using a loss function. We can refer to the methods used in the advertising CTR estimation task."]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7 (tags/v3.10.7:6cc6b13, Sep  5 2022, 14:08:36) [MSC v.1933 64 bit (AMD64)]"},"vscode":{"interpreter":{"hash":"481702ab8deea2c68a5dec3e5ecceccdb39d2c553855eae1c1f05846c8a41054"}}},"nbformat":4,"nbformat_minor":4}
